# Project rules for Cursor in `crawler_agent`

## Mission
- Build and expose Crawl4AI functionality as MCP tools consumable by OpenAI Agents SDK, Cursor, and Cloud Code.
- Deliver small, safe, reversible changes with high reliability and clear observability.

## Operating mode
- Be a senior engineer: propose a short plan, then execute.
- Prefer minimal diffs; change only what’s needed; avoid unrelated reformatting.
- Preserve existing indentation and style; match the surrounding code.
- Default to strict typing, meaningful names, small functions, early returns.

## Workflow
- Before coding: confirm scope in a numbered plan; call out risky edits for explicit approval.
- Write/expand tests when feasible; keep coverage meaningful (no performative tests).
- After edits: run the smallest checks (build/test/lint) required to validate; capture output.
- Summarize changes and next steps succinctly.

## Tooling rules (Cursor agents)
- Use the repository’s docs for authoritative references:
  - Crawl4AI: `https://docs.crawl4ai.com/`
  - OpenAI Agents SDK MCP: `https://openai.github.io/openai-agents-python/mcp/`
  - Tools reference: `https://openai.github.io/openai-agents-python/tools/`
  - Agent reference (mcp_servers, mcp_config): `https://openai.github.io/openai-agents-python/ref/agent/`
  - MCP intro: `https://modelcontextprotocol.io/introduction`
- When reading local docs:
  - Prefer targeted searches. For `docs/modelcontextprotocol-python-sdk-CLAUDE-MD.txt`, read specific sections only (never the entire file).
  - Use grep-style queries to locate symbols (e.g., `Server(`, `@server.call_tool`, `MCPServerStdio`).
- Parallelize read-only queries to gather context quickly; sequence only when output A is needed for B.

## Design constraints
- Transport: start with stdio MCP server; consider SSE/Streamable HTTP later.
- Contracts: strict, versioned JSON schemas (Pydantic) for tool inputs/outputs.
- Observability: structured logs, tracing spans for `list_tools`/`call_tool`, basic counters and latencies.
- Safety: domain allow/deny lists, robots.txt modes, max depth/TTL/timeouts, proxy/auth guards.
- Performance: reuse browser contexts where possible; cache results (opt-in); avoid deep re-crawls by default.

## Coding conventions
- Python 3.11+; explicit type hints for public APIs; no bare `except`; meaningful error messages.
- Avoid deep nesting; guard clauses first; do not over-catch exceptions.
- Add concise docstrings for public functions/classes; explain intent, not obvious mechanics.
- Do not add comments to explain trivial code; keep code self-documenting.

## Edit discipline
- Do not rename or move files unless requested or essential; if so, include a rollback note.
- Keep unrelated formatting unchanged. Do not mass-reflow lines or reorder imports unless necessary.
- Gate new dependencies behind explicit confirmation; pin versions when adding.

## Security & privacy
- Never enable crawling of internal networks, localhost, or file:// schemes.
- Respect site policies; provide robots.txt compliance modes.
- Redact secrets; never log credentials or PII; sanitize outputs forwarded to LLMs.

## MCP + Crawl4AI specifics
- Tools to expose initially: `crawl_url`, `deep_crawl`, `adaptive_crawl` (small, composable surface).
- Map args to Crawl4AI configs (`crawler_config`, `browser_config`, optional C4A-Script).
- Outputs: structured JSON with `markdown`, `links`, `metadata`; consider chunking options.
- Strict schemas: enable strict argument validation and deterministic tool behavior.

## Validation steps (typical)
- Lint/tests/build minimal run; for MCP stdio server, a smoke run that lists tools and runs `crawl_url` against a simple page.
- Capture outputs and errors; fail fast with actionable messages.

## Documentation & citations
- When referencing external content, cite links inline using Markdown.
- Update project docs/runbooks when changing public contracts, transports, or observability.

## Commit etiquette
- Clear, scoped commit messages; include rationale and any rollout/rollback notes when changing contracts or infra.
